{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Student_DoNotEdit_VizWizSection2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkateshgaur/game/blob/master/Student_DoNotEdit_VizWizSection2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4sLmjIuDpmY"
      },
      "source": [
        "![](https://vizwiz.org/wp-content/uploads/2019/05/logo-name1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyvASmeODyC3"
      },
      "source": [
        "In the previous week, we started our Vizwiz project and got our data ready. \n",
        "In this week, we train our own LSTM-based models. \n",
        "\n",
        "We'll start by visualizing our data, then identifying the best algorithm for our problem. In order to do so, we will explore a few model variants.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMoymbJeqib4"
      },
      "source": [
        "#@title Run this to download data and prepare our environment! { display-mode: \"form\" }\n",
        "\n",
        "\n",
        "### preparing definitions\n",
        "# downloading and loading data\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import gdown\n",
        "\n",
        "# file variables\n",
        "metadata_url         = 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/Deep%20Dives/Advanced%20Topics%20in%20AI/Sessions%206%20-%2010%20(Projects)/Project%20-%20Visual%20Question%20Answering/Annotations.zip'\n",
        "metadata_path        = './metadata.zip'\n",
        "\n",
        "###\n",
        "gdown.download(metadata_url, './metadata.zip', True)\n",
        "!unzip ./metadata.zip\n",
        "\n",
        "# file variables\n",
        "data_url         = 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/Deep%20Dives/Advanced%20Topics%20in%20AI/Sessions%206%20-%2010%20(Projects)/Project%20-%20Visual%20Question%20Answering/small.h5'\n",
        "image_data_path      = 'small.h5'\n",
        "\n",
        "###\n",
        "gdown.download(data_url, 'small.h5', True)\n",
        "\n",
        "# file variables\n",
        "train_url         = 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/Deep%20Dives/Advanced%20Topics%20in%20AI/Sessions%206%20-%2010%20(Projects)/Project%20-%20Visual%20Question%20Answering/train.h5'\n",
        "train_path      = 'train.h5'\n",
        "###\n",
        "gdown.download(train_url, 'train.h5', True)\n",
        "\n",
        "# file variables\n",
        "val_url         = 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/Deep%20Dives/Advanced%20Topics%20in%20AI/Sessions%206%20-%2010%20(Projects)/Project%20-%20Visual%20Question%20Answering/val.h5'\n",
        "val_path      = 'val.h5'\n",
        "\n",
        "###\n",
        "gdown.download(val_url, 'val.h5', True)\n",
        "\n",
        "\n",
        "\n",
        "# file variables\n",
        "data_url         = 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/Deep%20Dives/Advanced%20Topics%20in%20AI/Sessions%206%20-%2010%20(Projects)/Project%20-%20Visual%20Question%20Answering/embedding_matrix.h5'\n",
        "image_data_path      = 'embedding_matrix.h5'\n",
        "\n",
        "###\n",
        "gdown.download(data_url, 'embedding_matrix.h5', True)\n",
        "embeddings_path = 'embedding_matrix.h5'\n",
        "\n",
        "\n",
        "\n",
        "data_url         = 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/Deep%20Dives/Advanced%20Topics%20in%20AI/Sessions%206%20-%2010%20(Projects)/Project%20-%20Visual%20Question%20Answering/vocab_vizwiz%20(1).json'\n",
        "image_data_path      = 'vocab.json'\n",
        "\n",
        "###\n",
        "gdown.download(data_url, 'vocab.json', True)\n",
        "vocab_path = 'vocab.json'\n",
        "\n",
        "\n",
        "import json\n",
        "import random\n",
        "\n",
        "def get_annotations(annotations_path):\n",
        "    with open(annotations_path, 'r') as f:\n",
        "      metadata = json.load(f)\n",
        "    return metadata\n",
        "\n",
        "def get_random_annotation(annotations_path):\n",
        "    with open(annotations_path, 'r') as f:\n",
        "      metadata = json.load(f)\n",
        "    i = random.choice(range(len(metadata)))\n",
        "    return metadata[i]\n",
        "\n",
        "# getting data\n",
        "annotations_path = './Annotations/val.json'\n",
        "vocab_path = 'vocab.json'\n",
        "\n",
        "def get_questions(annotations_path):\n",
        "  \n",
        "  with open(annotations_path, 'r') as f:\n",
        "    annos = json.load(f)\n",
        "\n",
        "  questions = set()\n",
        "  for i in range(len(annos)):\n",
        "    if annos[i]['answerable'] == 0:\n",
        "      continue\n",
        "    question = annos[i]['question']\n",
        "    questions.add(question)\n",
        "\n",
        "  return questions\n",
        "  \n",
        "from collections import Counter \n",
        "  \n",
        "def most_frequent(List): \n",
        "    occurence_count = Counter(List) \n",
        "    return occurence_count.most_common(1)[0][0] \n",
        "\n",
        "def get_answers(annotations_path):\n",
        "  with open(annotations_path, 'r') as f:\n",
        "    annos = json.load(f)\n",
        "  answers = set()\n",
        "  for i in range(len(annos)):\n",
        "    if annos[i]['answerable'] == 0:\n",
        "      continue\n",
        "    ans = annos[i]['answers']\n",
        "    answers_list = []\n",
        "    for a in ans:\n",
        "      answers_list += [a['answer']]\n",
        "    answers.add(most_frequent(answers_list))\n",
        "  return answers\n",
        "\n",
        "def get_image_paths(annotations_path):\n",
        "  with open(annotations_path, 'r') as f:\n",
        "    annos = json.load(f)\n",
        "\n",
        "  images = set()\n",
        "  for i in range(len(annos)):\n",
        "    if annos[i]['answerable'] == 0:\n",
        "      continue\n",
        "    image = annos[i]['image']\n",
        "    images.add(image)\n",
        "\n",
        "  return images\n",
        "\n",
        "def parse_vocab(q, a):\n",
        "  vocab = set()\n",
        "  for qu in q:\n",
        "    if '?' in qu:\n",
        "      qu = qu.replace('?', '')\n",
        "\n",
        "    vocab |= set(qu.split(' '))\n",
        "  return vocab\n",
        "\n",
        "## data and ploting\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_data(path):\n",
        "    data = h5py.File(path, 'r')\n",
        "    images = data['images']\n",
        "    image_idx = data['image_indices']\n",
        "    questions = data['questions']\n",
        "    answers = data['answers']\n",
        "    return images, image_idx, questions, answers\n",
        "\n",
        "def plot_image(img, questions=None, answers=None, indices=None):\n",
        "    if indices is None:\n",
        "      plt.imshow(img/255)\n",
        "      plt.show()\n",
        "    else:\n",
        "      for i in indices:\n",
        "        print('Question: ', questions[i])\n",
        "        print('Answer: ', answers[i])\n",
        "        plt.imshow(img[i]/255)\n",
        "        plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4r1_Pc3vEKD"
      },
      "source": [
        "# Milestone 1. Understand and visualizing our data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbjXNQ0b6fZm"
      },
      "source": [
        "\n",
        "## Activity 1a. What data do we have?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4xmAl3AvlJw"
      },
      "source": [
        "### Exercise (Coding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CkYJnMCBFJX"
      },
      "source": [
        "We use the `get_random_annotation(annotations_path)` function to show the format of a visual question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSkCbAg7Jdvt"
      },
      "source": [
        "# get a table with information about our visual questions\n",
        "metadata = get_random_annotation(annotations_path)\n",
        "\n",
        "# what does it look like?\n",
        "metadata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUdkpKIIGQW7"
      },
      "source": [
        "## Let's take a look at our preprocessed data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOCUsF5K9sA7"
      },
      "source": [
        "We'll start by loading a subset of our data using `get_data`. \n",
        "\n",
        "\n",
        "`get_data` is a function that **loads** our images and visual questions\n",
        "\n",
        "```\n",
        "images, image_indices, questions, answers = get_data()\n",
        "```\n",
        "\n",
        "`images` is a numpy array of our images, with size `(number of images, 224, 224, 3)`. \n",
        "\n",
        "`image_indices` is a numpy array of our images indices for each q-a pair, with size `(number of q-a pairs,)`.\n",
        "\n",
        "`questions` is a list of questions, with size `(number of q-a pairs, )`.\n",
        "\n",
        "`answers` is a list of answers, with size `(number of q-a pairs, )`. \n",
        "\n",
        "**Let's try loading our data!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJEKBKd69uhT"
      },
      "source": [
        "images, image_idx, questions, answers = get_data('small.h5')\n",
        "img_idx = image_idx[0]\n",
        "image = images[img_idx]\n",
        "question = questions[0]\n",
        "answer = answers[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TkTcYRC94Dj"
      },
      "source": [
        "print('Our inputs are image question pairs with the output being the answer!')\n",
        "print('Our image is stored as %s in Python'%type(image))\n",
        "print('Our image has dimensions of (%d, %d, %d)'%image.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at6q_Li7zJXi"
      },
      "source": [
        "**Visualize single data points with plot_image**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WxXax6-FkMj"
      },
      "source": [
        "Let's see a single data point. \n",
        "\n",
        "`plot_image` can take in either one image or many images. To use it with one image that is 3D, call:\n",
        "\n",
        "```\n",
        "plot_image(image)\n",
        "```\n",
        "\n",
        "To use it with many images, call:\n",
        "\n",
        "\n",
        "```\n",
        "plot_image(images, questions, answers, indices)\n",
        "```\n",
        "\n",
        "where:\n",
        "* `images`: all of our images in one array\n",
        "* `questions`: the questions corresponding to the images\n",
        "* `answers`: the answers corresponding to the image-question pairs\n",
        "* `indices`: which images we want to see\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJibNL7CzXrl"
      },
      "source": [
        "# plot a SINGLE image\n",
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmaAKTgF6lJI"
      },
      "source": [
        "\n",
        "# Milestone 2. Language only: LSTM-only model for our VQA data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA2tfXWP6lq5"
      },
      "source": [
        "\n",
        "## Activity 2a. Defining our machine learning problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgcE5wc7-MYM"
      },
      "source": [
        "We explore our problem in terms of language only models. \n",
        "We train a ***bare LSTM model*** and see its performance in our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke6JF4cQ88dA"
      },
      "source": [
        "### Exercise (Discussion) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z457gSlfVurn"
      },
      "source": [
        "What WILL our model look like? Let's define our **inputs** and **outputs**.\n",
        "\n",
        "Will a bare LSTM model perform well on this?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx5vAivA9bLK"
      },
      "source": [
        "#@title How will our model operate? { display-mode: \"form\" }\n",
        "\n",
        "#@markdown What are our inputs? \n",
        "inputs = \"questions only\" #@param [\"images+questions\", \"images only\", \"questions only\", \"answers\", \"FillMe\"]\n",
        "\n",
        "#@markdown What are our outputs? \n",
        "outputs = \"answers\" #@param [\"images+questions\", \"images\", \"image edges\", \"answers\", \"FillMe\"]\n",
        "\n",
        "\n",
        "#@markdown Will a bare LSTM do well in this task? \n",
        "problem_type  = \"yes\" #@param [\"yes\", \"no\", \"choose answer\"]\n",
        "\n",
        "\n",
        "if inputs == \"questions only\":\n",
        "  print(\"Yes, in this specific case our inputs are only questions!\")\n",
        "else:\n",
        "  print('Not quite our inputs.')\n",
        "\n",
        "if outputs == \"answers\":\n",
        "  print(\"Yes, our outputs are answers!\")\n",
        "else:\n",
        "  print('Not quite our outputs.')\n",
        "  \n",
        "if problem_type == \"choose answer\":\n",
        "  print(\"Choose an answer por favor!\")\n",
        "else:\n",
        "  print('Well! Let\\'s train such a model and then we can answer this question.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLGqxv3UwYnh"
      },
      "source": [
        "## Activity 2b. Building our model\n",
        "\n",
        "As we said in the previous exercise, our data points will be composed of preprocessed questions and answers without the images. \n",
        "\n",
        "We now build our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVpNck1f6mMZ"
      },
      "source": [
        "### Exercise (Coding)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeucolNdKAVP"
      },
      "source": [
        "#@title Run this to prepare our functions! { display-mode: \"form\" }\n",
        "\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, Activation, Dropout, LSTM, Flatten, Embedding, multiply\n",
        "import h5py\n",
        "\n",
        "def BareLSTMModel(embedding, vocab_size, q_lengths = 20, \n",
        "                  num_layers = 2, hidden_size=512, \n",
        "                  embedding_size=100, num_classes = 30,\n",
        "                  dropout_rate=0.5):\n",
        "\n",
        "  language_model = Sequential()\n",
        "  language_model.add(Embedding(vocab_size, embedding_size, \n",
        "                    weights=[embedding], input_length=q_lengths, \n",
        "                    trainable=False))\n",
        "  language_model.add(LSTM(hidden_size, return_sequences=True, \n",
        "                          input_shape=(q_lengths, embedding_size)))\n",
        "  for i in range(num_layers-2):\n",
        "      language_model.add(LSTM(hidden_size, return_sequences=True))\n",
        "  language_model.add(LSTM(hidden_size, return_sequences=False))\n",
        "  language_model.add(Dropout(dropout_rate))\n",
        "  language_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "  return language_model\n",
        "\n",
        "\n",
        "#cite Ranjay's IQ paper for code\n",
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "\n",
        "class Vocabulary(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Constructor for Vocabulary.\n",
        "        \"\"\"\n",
        "        # Init mappings between words and ids\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "        self.add_word('<pad>')\n",
        "        self.add_word('<start>')\n",
        "        self.add_word('<end>')\n",
        "        self.add_word('<unk>')\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "          self.word2idx[word] = self.idx\n",
        "          self.idx2word[self.idx] = word\n",
        "          self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            return self.word2idx['<unk>']\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "    def save_vocab(self, location):\n",
        "        with open(location, 'w') as f:\n",
        "            json.dump({'word2idx': self.word2idx,\n",
        "                       'idx2word': self.idx2word,\n",
        "                       'idx': self.idx}, f)\n",
        "\n",
        "    def load_vocab(self, location):\n",
        "        with open(location, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            self.word2idx = data['word2idx']\n",
        "            self.idx2word = data['idx2word']\n",
        "            self.idx = data['idx']\n",
        "\n",
        "    def decode_sentence(self, tokens):\n",
        "        words = []\n",
        "        for token in tokens:\n",
        "            word = self.idx2word[str(token)]\n",
        "            if word == '<end>':\n",
        "                break\n",
        "            if word not in ['<pad>', '<start>', \n",
        "                            '<end>', '<unk>']:\n",
        "                words.append(word)\n",
        "        return ' '.join(words)\n",
        "\n",
        "def load_vocab(vocab_path):\n",
        "    vocab = Vocabulary()\n",
        "    vocab.load_vocab(vocab_path)\n",
        "    return vocab\n",
        "\n",
        "def get_embeddings(path):\n",
        "  matrix = h5py.File(path, 'r')\n",
        "  return matrix['embedding_matrix'][()]\n",
        "\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "def prepare_data(data_path):\n",
        "    data = h5py.File(data_path, 'r')\n",
        "\n",
        "    questions = data['questions'][()]\n",
        "    answers = to_categorical(data['answers'][()])\n",
        "    return questions, answers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93WOHcjxsWHT"
      },
      "source": [
        "In order to build our model, we use the following function:\n",
        "\n",
        "`BareLSTMModel(embeddings, vocab_size, embedding_dim)`\n",
        "\n",
        "where `embeddings` is our embedding matrix, `vocab_size` is our vocabulary size and `embedding_dim` represents our embedding dimensions. \n",
        "\n",
        "Before we do so, we need to load our embedding matrix and our vocab.\n",
        "\n",
        "Make sure to use `get_embeddings(embeddings_path)` to load the embedding matrix and `load_vocab(vocab_path)` for our vocab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piJP1XsfU9w9"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOwGddsZXDfr"
      },
      "source": [
        "#### Visualizing our embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc85A8G_XCrc",
        "cellView": "form"
      },
      "source": [
        "#@title Choose an index in our vocab\n",
        "\n",
        "index = \"566\" #@param {type:\"string\"}\n",
        "\n",
        "if int(index) < vocab.idx:\n",
        "  print(\"Our original word is: %s\"%(vocab.idx2word[index]))\n",
        "  print(\"The given embbeding is: \", embedding_matrix[int(index)])\n",
        "else:\n",
        "  print(\"Choose an index less than %d\"%vocab.idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMD67J3MIZK6"
      },
      "source": [
        "####Let's build our model now!\n",
        "\n",
        "Use `BareLSTMModel(embedding, len(vocab))` and make sure to specify the right parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tySX5E1nIYW1",
        "cellView": "both"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpFscgpDdLck"
      },
      "source": [
        "Let's visualize our model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRQZ2a5fdP6p"
      },
      "source": [
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEKcyS1625HM"
      },
      "source": [
        "## Activity 2c. Preparing our data\n",
        "\n",
        "With our model ready, we need to prepare our questions and answers.\n",
        "\n",
        "In order to do so, we use the following function:\n",
        "\n",
        "```tokenized_questions, tokenized_answers = prepare_data(train_path)```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAkpFExw4R1k"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTrPEU9zV7Oe"
      },
      "source": [
        "##Activity 2d. Training our model\n",
        "\n",
        "We built our model and prepared our data. Now, let's train our model!\n",
        "\n",
        "We use `model.fit`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbD50xcwY-ue"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7Kepm8MomEE"
      },
      "source": [
        "## Activity 2e. Evaluating our model\n",
        "\n",
        "After training our model, we now evaluate it on our validation set.\n",
        "\n",
        "We specify our validation set path in `val_path`. \n",
        "\n",
        "Make sure you prepare the data using the `prepare_data(path)` function and evaluate our model using `model.evaluate` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xPExvGRos3z"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5cW-2jqkLjg"
      },
      "source": [
        "Make sure to run the model with different parameters and more epochs to get better performance!\n",
        "\n",
        "## Discussion\n",
        "How good do you think the model is?\n",
        "\n",
        "Can this model be a reliable asset for the blind?\n",
        "\n",
        "Do you think we can do better with ***image data***? and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUCGQFdno3AG"
      },
      "source": [
        "## (Optional Activity) Let's finetune our model!\n",
        "\n",
        "Try to train our model with a different set of parameters in order to maximize our performance. \n",
        "\n",
        "We specify the variable that will be tuned. Make sure to change these values to improve our model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1qEXBmppv4K"
      },
      "source": [
        "model_weights_filename = 'bareLSTM.h5'\n",
        "### MAKE SURE TO CHANGE THE VARIABLES BELOW!!!\n",
        "epochs = 10 # number of epochs\n",
        "num_layers = 1 # number of RNN layers\n",
        "hidden_size = 256 # size of each layer\n",
        "batch_size = 64 # batch size\n",
        "dropout_rate = 0.5 # Dropout rate\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8LWHmRZpwAs"
      },
      "source": [
        "#@title Run this to initialize and run our model! { display-mode: \"form\" }\n",
        "### YOUR CODE HERE\n",
        "embedding_dim = 100\n",
        "model = BareLSTMModel(embedding_matrix, len(vocab),\n",
        "                      num_layers=num_layers, \n",
        "                      hidden_size=hidden_size,\n",
        "                      dropout_rate= dropout_rate)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "model.fit(questions, answers, epochs=epochs, batch_size=batch_size, shuffle=True)\n",
        "model.save_weights(model_weights_filename, overwrite=True)\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpNOf6tXFe7B"
      },
      "source": [
        "# Milestone 3: Using image information in our model\n",
        "\n",
        "After training our bare LSTM, we now use a CNN-LSTM model to answer our questions. The CNN-LSTM model encodes the information contained in the image and uses it to answer our questions.\n",
        "\n",
        "As the information contained in the image is necessary to answer these questions, we should expect this model to do better than the previous one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJMEfdFAteH5"
      },
      "source": [
        "#@title Run this to download data and prepare our environment! { display-mode: \"form\" }\n",
        "\n",
        "\n",
        "### preparing definitions\n",
        "# downloading and loading data\n",
        "import gdown\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "\n",
        "# file variables\n",
        "train_feats_url         = 'https://drive.google.com/uc?id=1u485ehCP-ccAt3sSPwjCE0qCK-dOVsl5'\n",
        "train_feats_path        = 'train_features.hdf5'\n",
        "\n",
        "gdown.download(train_feats_url, 'train_features.hdf5', False)\n",
        "\n",
        "# file variables\n",
        "\n",
        "# file variables\n",
        "val_feats_url         = 'https://drive.google.com/uc?id=1cdXPB49VuLr4ZbH-bwza9orlDddwDYOr'\n",
        "val_feats_path        = 'val_features.hdf5'\n",
        "\n",
        "gdown.download(val_feats_url, 'val_features.hdf5', False)\n",
        "\n",
        "def encode_image(images_path, data_path):\n",
        "  image_data = h5py.File(images_path, 'r')\n",
        "  data = h5py.File(data_path, 'r')\n",
        "  questions = data['questions'][()]\n",
        "  image_features = image_data['feats'][()]\n",
        "  answers = to_categorical(data['answers'][()])\n",
        "  return image_features, questions, answers\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Reshape\n",
        "from keras import Input\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers import concatenate\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.models import model_from_json, Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from collections import defaultdict\n",
        "import operator\n",
        "from keras.utils import np_utils, generic_utils\n",
        "from progressbar import Bar, ETA, Percentage, ProgressBar\n",
        "from itertools import zip_longest\n",
        "from keras.models import load_model\n",
        "\n",
        "def image_model(hidden_size=512):\n",
        "  image_model = Sequential()\n",
        "  image_model.add(Reshape(input_shape = (4096,), target_shape=(4096,)))\n",
        "  image_model.add(Dense(hidden_size, activation = 'tanh'))\n",
        "  return image_model\n",
        "\n",
        "def language_model(embedding, vocab_size, q_lengths = 20, \n",
        "                   num_layers = 2, hidden_size=512, \n",
        "                   embedding_size=100):\n",
        "\n",
        "  language_model = Sequential()\n",
        "  language_model.add(Embedding(vocab_size, embedding_size, \n",
        "                    weights=[embedding], input_length=q_lengths, \n",
        "                    trainable=False))\n",
        "  language_model.add(LSTM(hidden_size, return_sequences=True, \n",
        "                          input_shape=(q_lengths, embedding_size)))\n",
        "  for i in range(num_layers-2):\n",
        "      language_model.add(LSTM(hidden_size, return_sequences=True))\n",
        "  language_model.add(LSTM(hidden_size, return_sequences=False))\n",
        "\n",
        "  return language_model\n",
        "\n",
        "def vqa_model(embedding, vocab_size, q_lengths = 20, \n",
        "              num_lstm_layers = 2, hidden_size=512, \n",
        "              embedding_size=100, num_classes=30):\n",
        "  image_model_ = image_model(hidden_size)\n",
        "\n",
        "  language_model_ = language_model(embedding, vocab_size, \n",
        "                                   q_lengths = q_lengths, \n",
        "                                   num_layers = num_lstm_layers, \n",
        "                                   hidden_size=hidden_size, \n",
        "                                   embedding_size=embedding_size)\n",
        "  \n",
        "  combined = multiply([image_model_.output, language_model_.output])\n",
        "\n",
        "  model = Dense(256, activation = 'tanh')(combined)\n",
        "  model = Dropout(0.5)(model)\n",
        "\n",
        "  model = Dense(256, activation = 'tanh')(model)\n",
        "  model = Dropout(0.5)(model)\n",
        "\n",
        "  model = Dense(128, activation = 'tanh')(model)\n",
        "  model = Dropout(0.5)(model)\n",
        "\n",
        "  model = Dense(num_classes)(model)\n",
        "  model = Activation(\"softmax\")(model)\n",
        "\n",
        "  model = Model(inputs=[image_model_.input, language_model_.input], outputs=model)\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3VXkLl6X3jd"
      },
      "source": [
        "## Activity 3a. Encoding our images\n",
        "Instead of training a CNN model from scratch, we usually use an imagenet pretrained model in our pipeline.\n",
        "\n",
        "This does not only improve our training marginally, but also increases the speed of our forward pass. \n",
        "\n",
        "Indeed, it is possible to prepare our image features in advance and only run our LSTM model after.\n",
        "\n",
        "In order to do so, we use the following function:\n",
        "\n",
        "```image_features, questions, answers = encode_image(train_feats_path, train_path)``` \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rGcU6bBhOB0"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHmHKaq8nAoM"
      },
      "source": [
        "## Activity 3b. Training our model\n",
        "\n",
        "Now that we have our image features ready, let's train our CNN-LSTM model.\n",
        "\n",
        "In order to do so, we build our model using \n",
        "\n",
        "`model = vqa_model(embedding_matrix, vocab_size, q_lengths = 20, \n",
        "          num_lstm_layers = num_layers, hidden_size=hidden_size, embedding_size=100,\n",
        "          num_classes=30)`\n",
        "\n",
        "Make sure to specify `vocab_size`, `num_layers` and `hidden_size`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waNLs360od3T"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4J5QHZjYore"
      },
      "source": [
        "Let's visualize our model architecture!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpqIWwmbYsgd"
      },
      "source": [
        "plot_model(vqa_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLex3HLWZilL"
      },
      "source": [
        "Let's train our model now!\n",
        "\n",
        "Use ```vqa_model.fit([train_features, train_questions], train_answers, epochs=num_epochs, batch_size=batch_size, shuffle=True)``` to train our vqa model.\n",
        "\n",
        "Make sure to specify `num_epochs` and `batch_size`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksbItD2GZtXL"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hpyjz-HpBVl"
      },
      "source": [
        "## Activity 3c. Evaluating our model\n",
        "Let's evaluate our model. \n",
        "\n",
        "Make sure to load our validation set using\n",
        "\n",
        "```val_features, val_questions, val_answers = encode_image(val_features_path, val_path)``` \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KUXsp1duKAA"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOAGJd2KuNnx"
      },
      "source": [
        "##Activity 3d. Comparing our models predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJCV8s-RbZJ1"
      },
      "source": [
        "### ***Discuss:*** \n",
        "\n",
        "Let's compare both model's performance. which of the models did better? \n",
        "\n",
        "Can you explain why image information is crucial in this task?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWcS8mxWcIJe"
      },
      "source": [
        "## Activity 3e. Visualizing a few examples\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OgNzFO2tDBk"
      },
      "source": [
        "#@title Run this to prepare data! { display-mode: \"form\" }\n",
        "### YOUR CODE HERE\n",
        "import gdown\n",
        "\n",
        "# file variables\n",
        "im_url         = 'https://drive.google.com/uc?id=1t2-Rhbts6NdDPaC64uF_Z-MHZXyCSxbX'\n",
        "im_path        = 'im.zip'\n",
        "\n",
        "gdown.download(im_url, 'im.zip', True)\n",
        "!unzip im.zip\n",
        "\n",
        "filter_url         = 'https://drive.google.com/uc?id=1Jc4Nff-5oPrjc3dwdB4eI8NZjiWT1VTJ'\n",
        "filter_path        = 'filter.json'\n",
        "\n",
        "gdown.download(filter_url, 'filter.json', True)\n",
        "\n",
        "answers_list = json.load(open('filter.json', 'r'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEaaCFrEySML"
      },
      "source": [
        "#@title Choose data point to visualize? { display-mode: \"form\" }\n",
        "\n",
        "inputs = \"VizWiz_val_00000123.jpg\" #@param [\"FillMe\", \"VizWiz_val_00000009.jpg\", \"VizWiz_val_00000011.jpg\", \"VizWiz_val_00000028.jpg\", \"VizWiz_val_00000037.jpg\", \"VizWiz_val_00000058.jpg\", \"VizWiz_val_00000075.jpg\", \"VizWiz_val_00000084.jpg\", \"VizWiz_val_00000090.jpg\", \"VizWiz_val_00000093.jpg\", \"VizWiz_val_00000095.jpg\", \"VizWiz_val_00000100.jpg\", \"VizWiz_val_00000101.jpg\", \"VizWiz_val_00000102.jpg\", \"VizWiz_val_00000105.jpg\", \"VizWiz_val_00000114.jpg\", \"VizWiz_val_00000123.jpg\"]\n",
        "\n",
        "img = [\"VizWiz_val_00000009.jpg\", \"VizWiz_val_00000011.jpg\", \n",
        "       \"VizWiz_val_00000028.jpg\", \"VizWiz_val_00000037.jpg\", \n",
        "       \"VizWiz_val_00000058.jpg\", \"VizWiz_val_00000075.jpg\", \n",
        "       \"VizWiz_val_00000084.jpg\", \"VizWiz_val_00000090.jpg\", \n",
        "       \"VizWiz_val_00000093.jpg\", \"VizWiz_val_00000095.jpg\", \n",
        "       \"VizWiz_val_00000100.jpg\", \"VizWiz_val_00000101.jpg\", \n",
        "       \"VizWiz_val_00000102.jpg\", \"VizWiz_val_00000105.jpg\",\n",
        "       \"VizWiz_val_00000114.jpg\", \"VizWiz_val_00000123.jpg\"]\n",
        "\n",
        "if inputs == \"FillMe\":\n",
        "  print(\"Please choose a valid data point\")\n",
        "\n",
        "index = img.index(inputs)\n",
        "print(type(val_questions[index]))\n",
        "print('Our question is:', vocab.decode_sentence(val_questions[index].astype(int)))\n",
        "print('Our original answer is:', answers_list[val_answers[index].argmax()])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "img=mpimg.imread(inputs)\n",
        "plt.imshow(img)\n",
        "\n",
        "print('Our bare LSTM model predicts:', answers_list[model.predict([val_questions[index: index + 1]]).argmax()])\n",
        "print('Our VQA model predicts:', answers_list[vqa_model.predict([val_features[index:index+1], val_questions[index:index+1]]).argmax()])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT3tkZl73CCB"
      },
      "source": [
        "#@title Ask any question you want about the image { display-mode: \"form\" }\n",
        "\n",
        "def encode(vocab, sentence):\n",
        "    tokens = [0] * 20\n",
        "    tokens[0] = vocab.word2idx['<start>']\n",
        "    for i in range(min(len(sentence), 19)):\n",
        "      if sentence[i] in vocab.word2idx:\n",
        "        s = sentence[i]\n",
        "      else:\n",
        "        s = '<unk>'\n",
        "      tokens[i+1] = vocab.word2idx[s]\n",
        "    tokens[i+1] = vocab.word2idx['<end>']\n",
        "    return np.array(tokens)\n",
        "\n",
        "\n",
        "Question = \"what is the color ?\" #@param {type:\"string\"}\n",
        "\n",
        "q = encode(vocab, Question)\n",
        "print('Our bare LSTM model predicts:', answers_list[model.predict(q.reshape(1,-1)).argmax()])\n",
        "print('Our VQA model predicts:', answers_list[vqa_model.predict([val_features[index:index+1], q.reshape(1,-1)]).argmax()])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGqKDb2wte_g"
      },
      "source": [
        "# Fin! Let's analyse our model predictions in the next section!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmJ_ycYE4Zpj"
      },
      "source": [
        "![](https://vizwiz.org/wp-content/uploads/2019/05/logo-name1.png)"
      ]
    }
  ]
}